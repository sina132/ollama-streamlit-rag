>pip install ollama chromadb nltk numpy
install ollama
pull a model

-----------------------------

modify ollama model:
making a Modelfile such as (refrence: https://ollama.readthedocs.io/en/modelfile/):

FROM deepseek-r1:1.5b
PARAMETER num_ctx 8096
SYSTEM You are a assistant

>ollama create custom-model-name -f ./Modelfile

-----------------------------

#chat in chunks
'''
from ollama import chat

stream = chat(
    model=MODEL,
    messages=[{'role': 'user', 'content': 'is mario a war hero?'}], //can have 'assistant' role for the ai
    stream=True,
)

for chunk in stream:
  print(chunk['message']['content'], end='', flush=True)
'''
#basic chat
'''
from ollama import chat

response = chat(model=MODEL, messages=[
  {
    'role': 'user',
    'content': 'Why is the sky blue?',
  },
])

print(response.message.content)
'''
#single time response:
'''
import ollama
output = ollama.generate(
  model=MODEL,
  prompt=f"Using this data: {data}. Respond to this prompt: {input}"
)

print(output['response'])
'''

arguments:
  generate:
    model: Specifies the model to use (e.g., smollm2:135m).
    prompt: The input text for which the model generates a response.
    suffix: Optional text to append after the response.
    images: List of base64-encoded images for multimodal models.
  
  chat:
    model: Specifies the model to use.
    messages: An array of message objects, each containing role (user or assistant) and content.
    stream: Optional; when true, streams the response as it's generated.
    format: Optional; specifies the format of the response.
    keep_alive: Optional; determines if the conversation context is maintained across multiple requests.
    tools: Optional; specifies any tools the model can use.
    options: Optional; a dictionary of additional settings like temperature, top_p, etc.
